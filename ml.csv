x,y
chat,<a href="https://chat.openai.com/auth/login?sso">chat</a>
user-guide, row=23   print(data.iloc[row  0])   data.iloc[row  1]
install packages, !pip install category_encoders     !pip install rfpimp    from rfpimp import *    import category_encoders as ce   from sklearn.model_selection import train_test_split   from sklearn.ensemble import RandomForestRegressor    from sklearn.metrics import mean_absolute_error r2_score
out-of-bag score, The out-of-bag (OOB) score is a measure of the performance of a random forest model that uses the samples that were not included in the bootstrap sample for training. This allows us to evaluate the model performance without the need for a separate validation set.
why how and where to use random_state, random_state is a parameter that is used in various machine learning models to set the random seed value. This means that if we set the same random_state value for a model. it will produce the same results every time we run it. The purpose of using random_state is to ensure that the results are reproducible and consistent across different runs of the model.
Understand the use of default_rng(seed=None) to create random data with and without a seed, The seed parameter in default_rng() is an optional argument that allows you to set a specific value to initialize the random number generator. This is useful when you need to generate the same sequence of random numbers multiple times. for example when testing or debugging your code. When you set a seed value. you ensure that the sequence of random numbers generated is always the same
Understand what train validation and test sets are used for., Train set: Subset of data used to train the model. The model is exposed to this data during training and learns to make predictions based on it. Validation set: Separate subset of data used to evaluate the model's performance during training. Used to tune hyperparameters and make decisions about model selection. Test set: Another separate subset of data used to evaluate the performance of the final model. Used to estimate the model's generalization performance on unseen data.
Understand how a random forest is created, Random forest is an ensemble learning method that combines multiple decision trees to create a more accurate and robust model.
Understand the steps of kNN, is a supervised machine learning algorithm used for classification and regression tasks. It works by finding the k-nearest data points in the training dataset to a given input point and classifying or regressing based on the labels of those k-nearest neighbors. 1)Store our training data 2)test data points 3)define distance 4)define k 5)calculate distances between datapoint and all trainng data 6)sort distances 7)choose closest k 8)calculate average of k and use of predict
How and why to prune decision trees, Pruning is a technique used in decision tree learning to reduce the size of the tree by removing nodes that provide little information gain or do not improve the overall accuracy of the model. The main idea behind pruning is to prevent overfitting which occurs when the model is too complex and captures noise in the training data.
Confusion matrix, A confusion matrix is a table that is used to evaluate the performance of a classification model. It compares the predicted classes of a model with the actual classes of the data. The matrix is arranged in a tabular format with rows representing the true classes and columns representing the predicted classes.
what is an out-of-bag sample and how is it used to estimate model error, An out-of-bag (OOB) sample is a data point that is not used in the bootstrap sample to train a random forest model and it is used to estimate the model's accuracy without the need for a separate validation set. OOB samples are used to calculate the OOB error which is an estimate of the model's generalization error.
Explain a generic learning process for supervised learning, A generic supervised learning process involves selecting a model splitting the data into training and testing sets training the model on the training set evaluating its performance on the testing set and tuning the hyperparameters if necessary. The goal is to find a model that can accurately predict the target variable on new unseen data.
What is overfitting, Overfitting is when a model learns the noise in the training data instead of the underlying pattern resulting in a model that performs well on the training data but poorly on new unseen data.Overfitting is when a model learns the noise in the training data instead of the underlying pattern resulting in a model that performs well on the training data but poorly on new unseen data.
Disadvantage of decision trees, Overfitting: Decision trees can easily overfit the training data which means that the tree is too complex and fits the noise in the data as well as the signal. This can result in poor generalization performance on new data. Instability: Small variations in the data can result in a completely different tree. This can make decision trees unstable and difficult to interpret. Bias: Decision trees can be biased towards features with more levels or features that appear earlier in the dataset.
what is R2, R-squared (R2) is a statistical measure that represents the proportion of the variance in the dependent variable that is explained by the independent variables in a regression model.
Hyperparameter vs Parameter, Parameters are learned by the model during training and represent the internal settings or variables that are optimized to improve the model's performance on the training data. In contrast hyperparameters are set by the user before training and control the behavior of the learning algorithm such as the speed of convergence or complexity of the model.
Cross validation," Cross-validation is a technique used to evaluate the performance of a machine learning model on an independent dataset. The idea behind cross-validation is to split the available data into several parts called ""folds"" and then use each fold in turn as a validation set while the rest of the data is used for training."
splitpoint, split_point = 42.5 df2 = pd.DataFrame({'x':X1 'y':y1}) df2 = df2.sort_values(by='x') print(df2) mean1=(4+12+1+9+7)/5 mae1=(abs(4-mean1)+abs(12-mean1)+abs(1-mean1)+abs(9-mean1)+abs(7-mean1))/5 mean2=(9+10)/2 mae2 = (abs(9-mean2)+abs(10-mean2))/2 print( mae1*5/7 + mae2*2/7 ).
Train a Model, Data must be numeric. No missing data. No outliers or inconsistency.
R2 Reminder, R2=1 means our model is perfect. R2~0 means our model does no better than just predicting the average. R2<0 means our model does worse than predicting the average. R2->1 it gets harder and harder to improve model performance.
Other Indicators, how much work the random forest model has to do to capture the relationship between the features and the target. the typical tree depth as this will impact the speed of predictions for new data. how important different features are for a given model.
Functions for evaluating model, one to evaluate our model and report the OOB score and the number of nodes across all trees in the forest and the median tree depth in the forest. one to show the feature importances for a given model.
Rfnnodes, Return the total number of decision and leaf nodes in all trees of the forest.
Rfmaxdepths, Return the max depth of all trees in rf forest in terms of how many nodes (a single root node for a single tree gives height 1).
Def evaluate," def evaluate(X y): rf = RandomForestRegressor(n_estimators=100 n_jobs=-1 oob_score=True) rf.fit(X y) oob = rf.oob_score_ n = rfnnodes(rf) h = np.median(rfmaxdepths(rf)) print(f""OOB R^2 is {oob:.5f} using {n:d} tree nodes with {h} median tree depth"") return rf oob"
Def showimp, def showimp(rf X y): features = list(X.columns) features.remove('latitude') features.remove('longitude') features += [['latitude' 'longitude']] I = importances(rf X y features=features) plot_importances(I color='#4575b4')
Feature Importance, Many times a model's ability to generalize (predict) well is not all we are hoping for we would also like to understand what the model is doing which is referred to as a model's interpretability. Random Forests have this as a built in feature however the implementation in sklearn suffers from bias when: the scales of the features vary and/or there are many categories for a feature. A better way to assess feature importance in any model is to use: permutation importance or dropped feature importance.
Permutation Importance, We can calculate the feature importances using a permutation method which consists of the following steps: 1.use all features and establish a baseline value for R2. 2.select one feature and randomly permute its values leaving all other features unchanged. 3.calculate the new value for R2 with this one feature permuted. 4.calculate the change in R2 from the baseline and. 5.repeat for the other features.
sample perm,# N - means same no. of rows unless you specify a new number. # Frac - means what fraction of rows you want to get returned e.g. 20%. 30%.... # Replace - means If n is greater than or less than the original data then. replace = True to get more rows than original. False means it will not replace and can't give you more than original no. of rows. # Axis - None means shuffling the rows # Axis = 1 means shuffling at column level.
Dropped Column Importance, We can also calculate the importance of the features using a dropped column which consists of the following steps: 1.use all features and establish a baseline value for R2. 2.select one feature and remove it from the data. 3.calculate the new value for R2 with this one feature removed. 4.calculate the change in R2 from the baseline and. 5.repeat for the other features.
Def perm importance, def perm_importances(X y): rf = RandomForestRegressor(n_estimators=100 n_jobs=-1 oob_score=True random_state=999) rf.fit(X y) r2 = rf.oob_score_ print(f"Baseline R^2 with no columns permuted: {r2:.5f}\n") for col in X.columns: X_col = X.copy() X_col[col] = X_col[col].sample(frac=1).values rf.fit(X_col y) r2_col = rf.oob_score_ print(f"Permuting column {col}: new R^2 is {r2_col:.5f} and difference from baseline is {r2 - r2_col:.5f}")
Def drom importance, def drop_importances(X y): rf = RandomForestRegressor(n_estimators=100 n_jobs=-1 oob_score=True random_state=999) rf.fit(X y) r2 = rf.oob_score_ print(f"Baseline R^2 with no columns dropped: {r2:.5f}\n") for col in X.columns: X_col = X.copy() X_col = X_col.drop(col axis=1) rf.fit(X_col y) r2_col = rf.oob_score_ print(f"Dropping column {col}: new R^2 is {r2_col:.5f} and difference from baseline is {r2 - r2_col:.5f}")
Categorical Variables, Ordinal: where there is some natural ordering to the categories. Nominal: where there is no natural ordering to the categories.
OrdinalEncoder - No Mapping, df = data.sample(n=421 random_state=45) feature = 'BldgType'  pip install category-encoders import category_encoders as ce  df_oridnal = ce.OrdinalEncoder(cols=['BldgType'])   df_oridnal.fit(df)   df = df_oridnal.transform(df)   df.iloc[331 :]
OrdinalEncoder - With Mapping, df = data.sample(n=447 random_state=80)   feature = 'Exterior1st'    mapping = {'AsbShng': 0  'BrkComm': 1  'BrkFace': 2 ......}     df_encoder = ce.OrdinalEncoder({'col': 'Exterior1st'      'mapping' : {'AsbShng': 0  'BrkComm': 1   'BrkFace': 2  }})     df_encoder.fit(df)    df = df_encoder.transform(df)      df.iloc[181   :]
OneHotEncoder, feature = 'Exterior1st'     df_encoder = ce.OneHotEncoder(cols=['Exterior1st'])    df_encoder.fit(df)    df = df_encoder.transform(df)   df.iloc[75  :]
CountEncoder/Frequency Encoder, feature = 'Heating'  df_encoder = ce.CountEncoder(cols=['Heating'])   df_encoder.fit(df)   df = df_encoder.transform(df)    df.iloc[249    :]
my_hoods, my_hoods = ['financial' 'gowanus']    hoods = {'hells': [40.7622 -73.9924]  'astoria': [40.7796684 -73.9215888] .....}     for hood in my_hoods:   rent[hood] = np.abs(rent['latitude'] - hoods[hood][0]) + np.abs(rent['longitude'] - hoods[hood][1])    rent.iloc[8641].values
TargetEncoder, rent_clean[['building_id' 'price']].groupby('building_id').mean()    encoder = ce.TargetEncoder(cols=['building_id'])     encoder.fit(X_train y_train)    X_train_enc = encoder.transform(X_train y_train)    y_train_enc = y_train.copy()    X_val_enc = encoder.transform(X_val)    y_val_enc = y_val.copy()
Feature Engineering, the process of transforming raw data into features that better represent the underlying problem to the predictive models resulting in improved model accuracy on unseen data. (Jason Brownlee). The goal is to discover (through domain knowledge experimentation etc.) which input features make it easier for the model to best predict the corresponding outputs. We won't cover all aspects of this topic but will cover a few approaches that should prove useful in your practical work.
Target Encoding, Any new feature that includes information about the target is referred to as target encoding. This is a technique that can sometimes be used to encode categorical variables. The simplest way to do this would be to use the mean of the target for each unique category value
Warning: Target Encoding, Using target information in our features is prone to overfitting so it is usually best to use a library like categorical_encoders to do this which is what we will do now. And to be careful we will use a validation set instead of the out-of-bag score.
MIN SAMPLE SPLIT, The minimum number of samples required to split an internal node: If int then consider min_samples_split as the minimum number. If float then min_samples_split is a fraction and ceil( min_samples_split n_samples are the minimum number of samples for each split.
MAX DEPTH, The maximum depth of the tree. If None then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.
min samples leaf, The minimum number of samples required to split an internal node: If int then consider min_samples_split as the minimum number. If float then min_samples_split is a fraction and ceil( min_samples_split n_samples are the minimum number of samples for each split.
max leaf nodes, Grow a tree with max_leaf_nodes in best first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.
