x,y
out-of-bag score, The out-of-bag (OOB) score is a measure of the performance of a random forest model that uses the samples that were not included in the bootstrap sample for training. This allows us to evaluate the model performance without the need for a separate validation set.
why how and where to use random_state, random_state is a parameter that is used in various machine learning models to set the random seed value. This means that if we set the same random_state value for a model. it will produce the same results every time we run it. The purpose of using random_state is to ensure that the results are reproducible and consistent across different runs of the model.
Understand the use of default_rng(seed=None) to create random data with and without a seed, The seed parameter in default_rng() is an optional argument that allows you to set a specific value to initialize the random number generator. This is useful when you need to generate the same sequence of random numbers multiple times. for example when testing or debugging your code. When you set a seed value. you ensure that the sequence of random numbers generated is always the same
Understand what train validation and test sets are used for., Train set: Subset of data used to train the model. The model is exposed to this data during training and learns to make predictions based on it. Validation set: Separate subset of data used to evaluate the model's performance during training. Used to tune hyperparameters and make decisions about model selection. Test set: Another separate subset of data used to evaluate the performance of the final model. Used to estimate the model's generalization performance on unseen data.
Understand how a random forest is created, Random forest is an ensemble learning method that combines multiple decision trees to create a more accurate and robust model.
Understand the steps of kNN, is a supervised machine learning algorithm used for classification and regression tasks. It works by finding the k-nearest data points in the training dataset to a given input point and classifying or regressing based on the labels of those k-nearest neighbors. 1)Store our training data 2)test data points 3)define distance 4)define k 5)calculate distances between datapoint and all trainng data 6)sort distances 7)choose closest k 8)calculate average of k and use of predict
How and why to prune decision trees, Pruning is a technique used in decision tree learning to reduce the size of the tree by removing nodes that provide little information gain or do not improve the overall accuracy of the model. The main idea behind pruning is to prevent overfitting which occurs when the model is too complex and captures noise in the training data.
Confusion matrix, A confusion matrix is a table that is used to evaluate the performance of a classification model. It compares the predicted classes of a model with the actual classes of the data. The matrix is arranged in a tabular format with rows representing the true classes and columns representing the predicted classes.
what is an out-of-bag sample and how is it used to estimate model error, An out-of-bag (OOB) sample is a data point that is not used in the bootstrap sample to train a random forest model and it is used to estimate the model's accuracy without the need for a separate validation set. OOB samples are used to calculate the OOB error which is an estimate of the model's generalization error.
Explain a generic learning process for supervised learning, A generic supervised learning process involves selecting a model splitting the data into training and testing sets training the model on the training set evaluating its performance on the testing set and tuning the hyperparameters if necessary. The goal is to find a model that can accurately predict the target variable on new unseen data.
What is overfitting, Overfitting is when a model learns the noise in the training data instead of the underlying pattern resulting in a model that performs well on the training data but poorly on new unseen data.Overfitting is when a model learns the noise in the training data instead of the underlying pattern resulting in a model that performs well on the training data but poorly on new unseen data.
Disadvantage of decision trees, Overfitting: Decision trees can easily overfit the training data which means that the tree is too complex and fits the noise in the data as well as the signal. This can result in poor generalization performance on new data. Instability: Small variations in the data can result in a completely different tree. This can make decision trees unstable and difficult to interpret. Bias: Decision trees can be biased towards features with more levels or features that appear earlier in the dataset.
what is R2, R-squared (R2) is a statistical measure that represents the proportion of the variance in the dependent variable that is explained by the independent variables in a regression model.
Hyperparameter vs Parameter, Parameters are learned by the model during training and represent the internal settings or variables that are optimized to improve the model's performance on the training data. In contrast hyperparameters are set by the user before training and control the behavior of the learning algorithm such as the speed of convergence or complexity of the model.
Cross validation, Cross-validation is a technique used to evaluate the performance of a machine learning model on an independent dataset. The idea behind cross-validation is to split the available data into several parts called "folds" and then use each fold in turn as a validation set while the rest of the data is used for training.