x,y
0, from keras.models import Sequential from keras.layers import Dense Flatten from keras.datasets import mnist
1, Optimizer:responsible for updating the model's weights during training to minimize the defined loss function. It determines how the model's parameters are adjusted in response to the training data and the calculated gradients.
2, Different optimizers have different algorithms for weight updates and they can impact the training process's speed and effectiveness. Common optimizers include stochastic gradient descent (SGD) Adam RMSprop and more.
3, Loss Function:(known as the objective or cost function) measures the discrepancy between the predicted values of the model and the actual target values in the training data. for regression is MSE quantifies the average squared difference between the predicted values and the actual target values. Classification: Cross-Entropy Loss measures the dissimilarity between predicted class probabilities and true class labels.
4, The choice of loss function depends on the nature of the problem you are solving. For example mean squared error (MSE) is commonly used for regression tasks while categorical cross-entropy is used for classification tasks.
5, Metrics: are used to evaluate the model's performance during training and can help you understand how well the model is doing on tasks beyond just minimizing the loss.
6, Gradiant Descent: fundamental optimization algorithm used in ML and DL to minimize a loss function and update the parameters (weights) of a model. 
7, Batch Gradient Descent: the entire training dataset is used to compute the gradient of the loss function with respect to the model parameters in each iteration.
8, Stochastic Gradient Descent (SGD): only one randomly selected training example is used to compute the gradient and update the model parameters in each iteration.
9, Mini-Batch Gradient Descent: strikes a balance between batch gradient descent and stochastic gradient descent. It divides the training dataset into smaller fixed-sized mini-batches.
10, The choice of which gradient descent variant to use depends on factors like the dataset size available computational resources and the specific problem being solved.
11, Epoch: is a complete pass through the entire training dataset during the training of a machine learning or deep learning model. In other words it's one full iteration over all training examples.
12, Batch Size: refers to the number of training examples used in each iteration (or mini-batch) during the training process.
13, The relationship between the number of epochs and the batch size determines how many parameter updates are made during training.
14, Learning Rate: It plays a fundamental role in determining the step size at which the model's parameters (weights and biases) are updated during training. t's a critical hyperparameter because it affects various aspects of the training process  including the convergence speed  stability  generalization  and the model's ability to find optimal solutions
15, Role of Learning: Control the Step Size Balance Between Speed and Stability Hyperparameter Tuning Adaptive Learning Rates Learning Rate Scheduling.
16, Network Used for Training: During the training phase the network is exposed to the training dataset which consists of input data and corresponding target labels.
17, Network Used for Testing: Once the training is completed the trained network is used for making predictions on new unseen data which is called the testing or inference phase.
18, Straightforward method for updating the weights in a neural network is the basic form of the stochastic gradient descent (SGD) optimization algorithm.
19, Initialize Weights-Iterative Weight Update-Forward Pass-Compute Loss-Backpropagation-Update Weights-Repeat.
20, Forward pass- input data is processed through the network's layers- and predictions or activations are computed layer by layer until the final output is generated. 
21, Backpropagation: It is the foundation for learning the optimal weights and biases that minimize the loss function and enable the network to make accurate predictions.
22, Fully Connected: it means that every neuron in one layer is connected to every neuron in the adjacent layer. This connectivity pattern is also known as a "dense" or "fully connected layer."
23, Deep learning is a subfield of machine learning that focuses on the use of artificial neural networks- specifically deep neural networks- to solve complex tasks. Deep neural networks are characterized by their deep architecture- which means they have multiple layers
24, In simple machine learning- such as traditional linear regression or decision tree models- the models typically have a shallow architecture with fewer layers. 
25, new_weight = old_weight - learning_rate * derivative
26, Gradiant: the gradient is a vector that contains the partial derivatives of the loss function with respect to the model's parameters. It guides the optimization process- allowing the model to iteratively update its parameters in a way that minimizes the loss and improves its predictive performance.
27, Vanishing gradient problem occurs when the gradients of the loss function with respect to the model's parameters become extremely small (close to zero) as they are backpropagated through the network during training.
28, Exploding gradient problem is the opposite of the vanishing gradient problem. It occurs when gradients become extremely large (growing exponentially) as they are backpropagated through the network.
29, dL/dw1 = (y_Pred - y_true) * (sigmoid(Z) * (1 - sigmoid(Z))) * x1
30, (train_images- train_labels)/ (test_images- test_labels) = mnist.load_data() / train_images = train_images.reshape((60000- 28 * 28)) / train_images = train_images.astype('float32') / 255 / test_images = test_images.reshape((10000- 28 * 28)) / test_images = test_images.astype('float32') / 255 / from keras.utils import to_categorical / train_labels = to_categorical(train_labels) / test_labels = to_categorical(test_labels) / network = Sequential() / network.add(Dense(512- activation='relu'- input_shape=(28 * 28-))) / network.add(Dense(10- activation='softmax')) / network.compile(optimizer='rmsprop'-loss='categorical_crossentropy'-metrics=['accuracy']) / network.fit(train_images- train_labels- epochs=7- batch_size=64)
31, np.random.seed(9261)   X1  y1 = np.random.randint(0  24  (3  4))  np.random.random(3)  W1 = [1 0 2 1] #weights  b1 = [-4] #bias z1 = np.dot(W1  X1.T) + b1 #fordware propagation funtion a1 = np.tanh(z1) #activation funtion  mse1 = np.mean((a1-y1)**2) #mean squared error
32, np.random.seed(789)   X2 = np.array([[2 -1 3] [-4 1 2]])   W2 = np.random.randint(-5  25 (1 3))   b2 = [1]     z2 = np.dot(W2  X2.T) + b     a2 = 1/(1+np.exp(-X2))
33, You get your data by first running np.random.seed(1230) and then creating the array X = np.random.randint(0- 255- (2932- 109- 99)). when the data is scaled and converted to the correct dimension for a fully connected network- what is the new 1160(st/nd/rd/th) pixel value for the 464(st/nd/rd/th) image?
34, np.random.seed(1230)  X = np.random.randint(0- 255- (2932- 109- 99))    print(X[463- 1160 // 99- 1160 % 99] / 255)  R=0.23137254901960785
35, You have image data that comes in an array of dimension (31511 47 58). This data needs to be fed into a fully-connected neural network.  What array dimensions does your data need?   R=(31511 2726)
36, A fully connected neural network is constructed with architecture the first hidden layer with 3 neurons.  the second hidden layer with 4 neurons and the third layer (output layer) contains 1 neuron.  it takes a input from a sample X where each sample contains 5 features. 
37, What would be the shape of the first hidden layers weight matrix given the representation of output from layer 1 is given by a[1] = sigma (W[1]]. x + B[1])) R= (3- 5)
38, when you add a dropout layer waht would be the output of the deactivated neuron during the forward pass? R=0
39, Short of what is defined as ReLu(x)=max(0 x)   R=Rectified linear unit
40, Assuming samples are represented as rows  use the following information to calculate the output of a single neuron with activation = ReLU X = np.array([[-4  -4  -4  1]  [-1  1  4  5]])  W = np.array([[-1  -2  3  2]])  b = np.array([[9]])
41, resp40: Z = np.dot(X  W.T) + b      r: [[11][30]]
42, If model y_hat = Sigmoid(w.X +b) initialized with w = 0.5 and b =1 . Loss function used for evaluating the model is Mean square error andgiven the gradient dertivate with respect to w (dl/dw) is given by -2 (y -y_hat)*y_hat (1-y_hat).x and the step size for each iteration is set to 0.5.What would be updated weight if the model trained with one sample X= 3 and y = 1 using stochastic gradient descent for 5 iteration (Note:Consider sample X is used in all iteration) ?
43, Resp42:  w=0.5 b=1 step_size=0.5 X=3 y=1 for _ in range(5): z=w*X+b  y_hat=1/(1+np.exp(-z))    gradient=-2*(y-y_hat)*y_hat*(1-y_hat)*X   w=w-step_size*gradient  print(w)     -0.8599
44, Derivative of the loss function provide? Gradient Information: The derivative provides information about how much the loss changes with respect to each model parameter (weights and biases) indicating the sensitivity of the loss to parameter adjustments. Direction of Update: The sign of the derivative tells us whether to increase or decrease each parameter to minimize the loss.
45, How many trainable parameters does the following network have: Input data has 532 features  layer 1 has 20 neurons  layer 2 has 6 neurons  layer 3 has 9 neurons  and the output layer has 6 neurons. 
46, resp45: inp=532 L1=20 L2=6 L3=9 out=6  par1=(inp*L1)+L1  par2=(L1*L2)+L2  par3=(L2*L3)+L3  par_out=(L3*out)+out  total_parameters=par1+par2+par3+par_out   10909
47, Assuming samples are represented as rows use the following information to calculate the output of a network with one hidden layer withactivations = ReLU X = np.array([[.   neuron1 = OurNeuralNetwork(W1 W2 b1 b2) print(neuron1.feedforward(X)).   change the activation code to:  return np.maximum(0 z)   R: [[10.31897597  5.         17.28213231]]
48, Output Shape:  Assume that the scaled input data of 5000 gray scale images...1st hidden layerwith 300 neurons 2nd hidden layer with 120 neurons 3rd hidden layer with 124 neurons. Which of the following Model summary given belowprovides the correct details output shape and params of the Network? (None 300) (None 120) (None 124)
49, Assume that the scaled input data of 5000 gray scale images are generated. X = np.random.randint(0 1 (5000 11 13))  y = np.random.randint(0 5 (5000))  network = Sequential() network.add(Flatten(input_shape=(11 13)))  network.add(Dense(300 activation='relu'))  network.add(Dense(120 activation='sigmoid'))  network.add(Dense(124 activation='sigmoid'))  network.compile(optimizer='rmsprop' loss='mean_squared_error' metrics=['accuracy'])  network.fit(X y epochs=7 batch_size=64)  network.summary()
50, Given the weight array for layer 3 W = np.array([[... how many weights will each neuron in layer 4 have:  print(W.shape[0])   R:20
